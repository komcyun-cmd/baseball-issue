import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import re

# --------------------------------------------------------------------------
# 1. ê°•ë ¥í•œ ìŠ¤í¬ë˜í¼ ì„¤ì • (ëª¨ë°”ì¼ ë¸Œë¼ìš°ì € ìœ„ì¥)
# --------------------------------------------------------------------------
st.set_page_config(page_title="Real-time KBO Monitor", layout="wide")

# ëª¨ë°”ì¼ User-Agent ì‚¬ìš© (PCë³´ë‹¤ ì°¨ë‹¨ í™•ë¥ ì´ í˜„ì €íˆ ë‚®ìŒ)
MOBILE_UA = {
    'User-Agent': 'Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
    'Referer': 'https://www.google.com'
}

# Cloudscraper ì¸ìŠ¤í„´ìŠ¤ (ë³´ì•ˆ ìš°íšŒìš©)
scraper = cloudscraper.create_scraper(browser={'browser': 'chrome', 'mobile': True})

# êµ¬ë‹¨ë³„ í‚¤ì›Œë“œ ë° ID ë§¤í•‘
TEAMS = {
    "í•œí™” ì´ê¸€ìŠ¤": {"dc_id": "hanwhaeagles_new", "keyword": "í•œí™”"},
    "KIA íƒ€ì´ê±°ì¦ˆ": {"dc_id": "tigers_new", "keyword": "ê¸°ì•„"},
    "ë¡¯ë° ìì´ì–¸ì¸ ": {"dc_id": "giants_new2", "keyword": "ë¡¯ë°"},
    "LG íŠ¸ìœˆìŠ¤": {"dc_id": "lgtwins_new", "keyword": "LG"},
    "ë‘ì‚° ë² ì–´ìŠ¤": {"dc_id": "doosanbears_new1", "keyword": "ë‘ì‚°"},
    "ì‚¼ì„± ë¼ì´ì˜¨ì¦ˆ": {"dc_id": "samsunglions_new", "keyword": "ì‚¼ì„±"},
    "SSG ëœë”ìŠ¤": {"dc_id": "wyverns_new", "keyword": "SSG"},
    "í‚¤ì›€ íˆì–´ë¡œì¦ˆ": {"dc_id": "heros_new", "keyword": "í‚¤ì›€"},
    "NC ë‹¤ì´ë…¸ìŠ¤": {"dc_id": "ncdinos", "keyword": "NC"},
    "KT ìœ„ì¦ˆ": {"dc_id": "ktwiz", "keyword": "KT"}
}

# --------------------------------------------------------------------------
# 2. ë‚ ì§œ í•„í„°ë§ ë¡œì§ (ë‹¨ìˆœí™” & ê°•í™”)
# --------------------------------------------------------------------------
def is_fresh(date_str):
    """
    XX:XX (ì˜¤ëŠ˜) -> ë¬´ì¡°ê±´ True
    MM.DD (ë‚ ì§œ) -> ì–´ì œ/ì˜¤ëŠ˜ì´ë©´ True, ì•„ë‹ˆë©´ False
    """
    date_str = date_str.strip()
    
    # 1. ì‹œê°„ìœ¼ë¡œ í‘œì‹œë˜ë©´ ì˜¤ëŠ˜ ê¸€ì„ (ì˜ˆ: 14:22)
    if ":" in date_str and len(date_str) < 8:
        return True
    
    # 2. ë‚ ì§œë¡œ í‘œì‹œë˜ë©´ (ì˜ˆ: 02.19 or 2024.02.19)
    try:
        # ìˆ«ìì™€ ì (.)ë§Œ ë‚¨ê¸°ê³  ì œê±°
        clean_date = re.sub(r'[^0-9.]', '', date_str)
        parts = clean_date.split('.')
        
        now = datetime.now()
        
        # ì—°ë„ê°€ ì—†ëŠ” ê²½ìš° (MM.DD)
        if len(parts) == 2:
            post_date = datetime(now.year, int(parts[0]), int(parts[1]))
        # ì—°ë„ê°€ ìˆëŠ” ê²½ìš° (YYYY.MM.DD)
        elif len(parts) == 3:
            year = int(parts[0])
            if year < 100: year += 2000 # 24.02.19 ëŒ€ì‘
            post_date = datetime(year, int(parts[1]), int(parts[2]))
        else:
            return False # í˜•ì‹ ë¶ˆëª…

        # 48ì‹œê°„ ì´ë‚´ ì²´í¬
        diff = now - post_date
        return diff.days <= 2
    except:
        return True # íŒŒì‹± ì—ëŸ¬ë‚˜ë©´ ì•ˆì „í•˜ê²Œ í¬í•¨

# --------------------------------------------------------------------------
# 3. í¬ë¡¤ë§ ì—”ì§„ (ì „ëµ ìˆ˜ì •ë¨)
# --------------------------------------------------------------------------

@st.cache_data(ttl=120)
def get_dc_mobile(team_name):
    """ì „ëµ: ëª¨ë°”ì¼ í˜ì´ì§€(m.dcinside) ì‚¬ìš© -> ì°¨ë‹¨ ìš°íšŒ ë° íŒŒì‹± ìš©ì´"""
    team_info = TEAMS.get(team_name)
    # ëª¨ë°”ì¼ìš© ì¶”ì²œ(ê°œë…)ê¸€ ëª©ë¡
    url = f"https://m.dcinside.com/board/{team_info['dc_id']}?recommend=1"
    
    try:
        resp = scraper.get(url, headers=MOBILE_UA, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ëª¨ë°”ì¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°
        items = soup.select('.gall-detail-lst li .subject')
        
        results = []
        for item in items:
            title_txt = item.select_one('.tit').text.strip()
            # ëª¨ë°”ì¼ ë‚ ì§œ: <span class="date">14:22</span>
            date_txt = item.select_one('.date').text.strip()
            
            if is_fresh(date_txt):
                link = item.get('href', '#') # ë§í¬ ì¶”ì¶œ
                # ë§í¬ê°€ ìƒëŒ€ê²½ë¡œì¸ ê²½ìš° ì²˜ë¦¬
                if not link.startswith('http'):
                    link = f"https://m.dcinside.com{link}"
                    
                results.append({'title': title_txt, 'link': link, 'date': date_txt})
                if len(results) >= 3: break
                
        return results if results else [{'title': '48ì‹œê°„ ë‚´ ê°œë…ê¸€ ì—†ìŒ', 'link': '#', 'date': '-'}]
    except Exception as e:
        return [{'title': f'DC ì ‘ì† ì‹¤íŒ¨: {e}', 'link': '#', 'date': 'Error'}]

@st.cache_data(ttl=120)
def get_mlb_filter(team_name):
    """ì „ëµ: ê²€ìƒ‰ ê¸°ëŠ¥ í¬ê¸° -> ìµœì‹ ê¸€ ëª©ë¡(3í˜ì´ì§€) ê¸ì–´ì„œ 'íŒ€ëª…' í•„í„°ë§ (ìµœì‹ ì„± ë³´ì¥)"""
    keyword = TEAMS[team_name]['keyword']
    base_url = "https://mlbpark.donga.com/mp/b.php?b=kbotown"
    
    results = []
    try:
        # 1~2í˜ì´ì§€ë§Œ ë¹ ë¥´ê²Œ ìŠ¤ìº”
        for page in range(1, 3):
            url = f"{base_url}&p={page * 30}" # ì— íŒ í˜ì´ì§• ê³„ì‚°
            resp = scraper.get(url, headers=MOBILE_UA, timeout=5)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            rows = soup.select('.tbl_type01 tbody tr')
            for row in rows:
                if 'notice' in row.get('class', []): continue # ê³µì§€ ì œì™¸

                title_tag = row.select_one('.tit a')
                date_tag = row.select_one('.date')
                
                if title_tag and date_tag:
                    title_txt = title_tag.text.strip()
                    date_txt = date_tag.text.strip()
                    
                    # 1. ë‚ ì§œ ë¨¼ì € ì²´í¬
                    if not is_fresh(date_txt): continue
                    
                    # 2. ì œëª©ì— íŒ€ ì´ë¦„ì´ ìˆëŠ”ì§€ ì²´í¬ (ì´ê²Œ í•µì‹¬)
                    if keyword in title_txt:
                        results.append({'title': title_txt, 'link': title_tag['href'], 'date': date_txt})
                        if len(results) >= 3: return results
                        
        return results if results else [{'title': f'{keyword} ê´€ë ¨ ìµœì‹ ê¸€ ì—†ìŒ', 'link': '#', 'date': '-'}]
    except Exception as e:
        return [{'title': f'ì— íŒ ì ‘ì† ì‹¤íŒ¨: {e}', 'link': '#', 'date': 'Error'}]

@st.cache_data(ttl=120)
def get_fmk_google_fallback(team_name):
    """ì „ëµ: í¨ì½” ì§ì ‘ ì ‘ì† ì‹œë„ -> ì‹¤íŒ¨ì‹œ êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©"""
    keyword = TEAMS[team_name]['keyword']
    
    # 1ì°¨ ì‹œë„: í¨ì½” í†µí•©ê²€ìƒ‰ (Cloudscraper)
    target_url = f"https://www.fmkorea.com/search.php?mid=baseball&search_keyword={keyword}&search_target=title_content"
    
    try:
        resp = scraper.get(target_url, headers=MOBILE_UA, timeout=5)
        
        # 403 Forbidden ë“± ì°¨ë‹¨ í™•ì¸
        if resp.status_code != 200 or "Cloudflare" in resp.text:
            raise Exception("Blocked")

        soup = BeautifulSoup(resp.text, 'html.parser')
        items = soup.select('.li.li_best2_pop0') # ì¸ê¸°ê¸€
        if not items: items = soup.select('.searchResult > li') # ì¼ë°˜ê¸€

        results = []
        for item in items:
            title_tag = item.select_one('dl > dt > a')
            time_tag = item.select_one('.time') or item.select_one('.regdate')
            
            if title_tag and time_tag:
                date_txt = time_tag.text.strip()
                if is_fresh(date_txt):
                    # ë§í¬ ì²˜ë¦¬
                    raw_link = title_tag['href']
                    link = f"https://www.fmkorea.com{raw_link}" if 'fmkorea' not in raw_link else raw_link
                    results.append({'title': title_tag.text.strip(), 'link': link, 'date': date_txt})
                    if len(results) >= 3: return results
        
        if results: return results

    except Exception:
        # 2ì°¨ ì‹œë„ (ì‹¤íŒ¨ ì‹œ): ê·¸ëƒ¥ ì—ëŸ¬ ë©”ì‹œì§€ ëŒ€ì‹  'ì§ì ‘ ë§í¬' ì œê³µ
        # êµ¬ê¸€ ê²€ìƒ‰ì„ ê¸ëŠ” ê±´ ë” ìœ„í—˜í•˜ë¯€ë¡œ ì‚¬ìš©ìì—ê²Œ ìš°íšŒ ë§í¬ ì œê³µì´ ê°€ì¥ í™•ì‹¤í•¨
        pass

    return [{'title': 'ğŸš« í¨ì½” ë³´ì•ˆ ì°¨ë‹¨ë¨ (í´ë¦­í•˜ì—¬ ì§ì ‘ ë³´ê¸°)', 'link': target_url, 'date': 'Link'}]


# --------------------------------------------------------------------------
# 4. UI ë Œë”ë§
# --------------------------------------------------------------------------
st.title("âš¾ KBO Radar (Final Ver.)")
st.markdown("---")

selected_team = st.selectbox("êµ¬ë‹¨ì„ ì„ íƒí•˜ì„¸ìš”", list(TEAMS.keys()))

if st.button("ìƒˆë¡œê³ ì¹¨ (ë°ì´í„° ê°€ì ¸ì˜¤ê¸°)", type="primary"):
    
    col1, col2, col3 = st.columns(3)
    
    # 1. DC Mobile
    with col1:
        st.subheader("ğŸ‘¿ DC (Mobile)")
        with st.spinner('DC ì ‘ì† ì¤‘...'):
            data = get_dc_mobile(selected_team)
            st.divider()
            for item in data:
                st.markdown(f"**[{item['title']}]({item['link']})**")
                st.caption(f"ğŸ•’ {item['date']}")
                st.write("")

    # 2. MLBPark Filter
    with col2:
        st.subheader("ğŸŸï¸ ì— íŒ (KBOíƒ€ìš´)")
        with st.spinner('ì— íŒ ìµœì‹ ê¸€ ìŠ¤ìº” ì¤‘...'):
            data = get_mlb_filter(selected_team)
            st.divider()
            for item in data:
                st.markdown(f"**[{item['title']}]({item['link']})**")
                st.caption(f"ğŸ•’ {item['date']}")
                st.write("")

    # 3. FMKorea (w/ Fallback)
    with col3:
        st.subheader("âš½ í¨ì½” (ì•¼êµ¬íƒ­)")
        with st.spinner('í¨ì½” ëš«ëŠ” ì¤‘...'):
            data = get_fmk_google_fallback(selected_team)
            st.divider()
            for item in data:
                if item['date'] == 'Link':
                    st.warning(f"[{item['title']}]({item['link']})")
                else:
                    st.markdown(f"**[{item['title']}]({item['link']})**")
                    st.caption(f"ğŸ•’ {item['date']}")
                st.write("")
