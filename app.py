import streamlit as st
import cloudscraper
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import pandas as pd

# --------------------------------------------------------------------------
# 1. ì„¤ì •: ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥í•˜ê¸° ìœ„í•œ Scraper ì„¤ì •
# --------------------------------------------------------------------------
st.set_page_config(page_title="KBO Hot Issue Monitor", layout="wide")

# Cloudscraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ë´‡ íƒì§€ ìš°íšŒìš©)
scraper = cloudscraper.create_scraper(
    browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
)

TEAMS = {
    "í•œí™” ì´ê¸€ìŠ¤": {"dc_id": "hanwhaeagles_new", "keyword": "í•œí™”"},
    "KIA íƒ€ì´ê±°ì¦ˆ": {"dc_id": "tigers_new", "keyword": "ê¸°ì•„"},
    "ë¡¯ë° ìì´ì–¸ì¸ ": {"dc_id": "giants_new2", "keyword": "ë¡¯ë°"},
    "LG íŠ¸ìœˆìŠ¤": {"dc_id": "lgtwins_new", "keyword": "LG"},
    "ë‘ì‚° ë² ì–´ìŠ¤": {"dc_id": "doosanbears_new1", "keyword": "ë‘ì‚°"},
    "ì‚¼ì„± ë¼ì´ì˜¨ì¦ˆ": {"dc_id": "samsunglions_new", "keyword": "ì‚¼ì„±"},
    "SSG ëœë”ìŠ¤": {"dc_id": "wyverns_new", "keyword": "SSG"},
    "í‚¤ì›€ íˆì–´ë¡œì¦ˆ": {"dc_id": "heros_new", "keyword": "í‚¤ì›€"},
    "NC ë‹¤ì´ë…¸ìŠ¤": {"dc_id": "ncdinos", "keyword": "NC"},
    "KT ìœ„ì¦ˆ": {"dc_id": "ktwiz", "keyword": "KT"}
}

# --------------------------------------------------------------------------
# 2. ë‚ ì§œ íŒŒì‹± ë¡œì§ (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„)
# --------------------------------------------------------------------------
def is_within_48_hours(date_text):
    """
    ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ ì²˜ë¦¬í•˜ì—¬ 48ì‹œê°„ ì´ë‚´ì¸ì§€ íŒë³„
    í˜•ì‹ ì˜ˆ: '14:22' (ì˜¤ëŠ˜), '02.18' (ì˜¬í•´), '2024.02.18' (ì „ì²´)
    """
    try:
        date_text = date_text.strip()
        now = datetime.now()
        post_date = None

        # Case 1: ì‹œê°„ë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: 14:22) -> ì˜¤ëŠ˜ ê²Œì‹œë¬¼
        if ":" in date_text and len(date_text) <= 5:
            return True
        
        # Case 2: ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° (ì˜ˆ: 02.18 or 2024.02.18)
        date_text = date_text.replace('-', '.').replace('/', '.') # êµ¬ë¶„ì í†µì¼
        
        parts = date_text.split('.')
        if len(parts) == 2: # MM.DD -> ì˜¬í•´ ì—°ë„ ë¶™ì„
            post_date = datetime(now.year, int(parts[0]), int(parts[1]))
        elif len(parts) == 3: # YYYY.MM.DD
            year = int(parts[0])
            # 2ìë¦¬ ì—°ë„(24.02.18)ì¸ ê²½ìš° ì²˜ë¦¬
            if year < 100: year += 2000 
            post_date = datetime(year, int(parts[1]), int(parts[2]))
            
        if post_date:
            diff = now - post_date
            return diff.days <= 2 # 48ì‹œê°„(2ì¼) ì´ë‚´
            
        return False # íŒŒì‹± ë¶ˆê°€ ì‹œ ì œì™¸
    except Exception:
        return True # ì—ëŸ¬ë‚˜ë©´ ì¼ë‹¨ ë³´ì—¬ì¤Œ (ì•ˆì „ì¥ì¹˜)

# --------------------------------------------------------------------------
# 3. í¬ë¡¤ë§ í•¨ìˆ˜ (Cloudscraper ì ìš©)
# --------------------------------------------------------------------------

@st.cache_data(ttl=300)
def get_dc_issues(team_name):
    team_info = TEAMS.get(team_name)
    # ì¼ë°˜ íƒ­ ëŒ€ì‹  'ê°œë…ê¸€' íƒ­ ì ‘ê·¼
    url = f"https://gall.dcinside.com/board/lists/?id={team_info['dc_id']}&exception_mode=recommend"
    
    try:
        # requests ëŒ€ì‹  scraper ì‚¬ìš©
        response = scraper.get(url, timeout=10)
        
        if response.status_code != 200:
            return [{'title': f"ì ‘ì† ì‹¤íŒ¨ (Code: {response.status_code})", 'link': '#'}]

        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select('tr.ub-content.us-post')
        
        results = []
        for row in rows:
            # ê³µì§€ì‚¬í•­ ì œì™¸
            if 'ub-notice' in row.get('class', []): continue

            title_tag = row.select_one('.gall_tit a')
            date_tag = row.select_one('.gall_date')
            
            if title_tag and date_tag:
                date_str = date_tag.get('title', date_tag.text.strip())
                if not is_within_48_hours(date_str): continue
                
                title = title_tag.text.strip()
                link = "https://gall.dcinside.com" + title_tag['href']
                results.append({'title': title, 'link': link})
                if len(results) >= 3: break
                
        return results if results else [{'title': "48ì‹œê°„ ë‚´ ì¸ê¸°ê¸€ ì—†ìŒ", 'link': '#'}]
    except Exception as e:
        return [{'title': f"ì—ëŸ¬: {str(e)}", 'link': '#'}]

@st.cache_data(ttl=300)
def get_mlbpark_issues(team_name):
    keyword = TEAMS[team_name]['keyword']
    url = f"https://mlbpark.donga.com/mp/b.php?b=kbotown&search_select=subject&search_input={keyword}"
    
    try:
        response = scraper.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select('.tbl_type01 tbody tr')
        
        results = []
        for row in rows:
            if 'notice' in row.get('class', []): continue
            
            title_tag = row.select_one('.tit a')
            date_tag = row.select_one('.date')
            
            if title_tag and date_tag:
                if not is_within_48_hours(date_tag.text.strip()): continue
                
                title = title_tag.text.strip()
                link = title_tag['href']
                results.append({'title': title, 'link': link})
                if len(results) >= 3: break
        return results if results else [{'title': "48ì‹œê°„ ë‚´ ì¸ê¸°ê¸€ ì—†ìŒ", 'link': '#'}]
    except Exception as e:
        return [{'title': f"ì—ëŸ¬: {str(e)}", 'link': '#'}]

@st.cache_data(ttl=300)
def get_fmkorea_issues(team_name):
    keyword = TEAMS[team_name]['keyword']
    # í¨ì½” í†µí•©ê²€ìƒ‰ URL
    url = f"https://www.fmkorea.com/search.php?mid=baseball&search_keyword={keyword}&search_target=title_content"
    
    try:
        response = scraper.get(url, timeout=10)
        
        if response.status_code != 200:
             return [{'title': f"í¨ì½” ì°¨ë‹¨ë¨ (Code: {response.status_code})", 'link': '#'}]

        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê²€ìƒ‰ ê²°ê³¼ ì„ íƒì (êµ¬ì¡° ë³€ê²½ ëŒ€ë¹„ 2ê°€ì§€ ì²´í¬)
        items = soup.select('.li.li_best2_pop0') # ì¸ê¸°ê¸€ ìŠ¤íƒ€ì¼
        if not items:
            items = soup.select('.searchResult > li') # ì¼ë°˜ ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
            
        results = []
        for item in items:
            # ë‚ ì§œ íƒœê·¸ í™•ì¸
            time_tag = item.select_one('.time')
            if not time_tag: time_tag = item.select_one('.regdate') # ë‹¤ë¥¸ í´ë˜ìŠ¤ëª… ëŒ€ë¹„

            title_tag = item.select_one('dl > dt > a')
            
            if title_tag and time_tag:
                if not is_within_48_hours(time_tag.text.strip()): continue
                
                title = title_tag.text.strip()
                href = title_tag['href']
                link = "https://www.fmkorea.com" + href if 'fmkorea' not in href else href
                
                results.append({'title': title, 'link': link})
                if len(results) >= 3: break
                
        return results if results else [{'title': "48ì‹œê°„ ë‚´ ì¸ê¸°ê¸€ ì—†ìŒ", 'link': '#'}]
    except Exception as e:
        return [{'title': f"ì—ëŸ¬: {str(e)}", 'link': '#'}]

# --------------------------------------------------------------------------
# 4. UI êµ¬ì„±
# --------------------------------------------------------------------------
st.title("âš¾ KBO Hot Issue (48h Real-time)")
st.caption("â€» ì—í¨ì½”ë¦¬ì•„/ë””ì‹œëŠ” ë³´ì•ˆì´ ê°•ë ¥í•˜ì—¬ ë¡œë”©ì— 3~5ì´ˆ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

selected_team = st.selectbox("êµ¬ë‹¨ ì„ íƒ", list(TEAMS.keys()))

if st.button("ìƒˆë¡œê³ ì¹¨", type="primary"):
    with st.spinner(f'{selected_team} ì´ìŠˆë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤... (Cloudscraper ì‘ë™)'):
        col1, col2, col3 = st.columns(3)
        
        # ë°ì´í„° ìˆ˜ì§‘
        dc = get_dc_issues(selected_team)
        mlb = get_mlbpark_issues(selected_team)
        fmk = get_fmkorea_issues(selected_team)
        
        # ê²°ê³¼ ì¶œë ¥ í•¨ìˆ˜
        def show_card(col, name, data, icon):
            with col:
                st.subheader(f"{icon} {name}")
                st.divider()
                for item in data:
                    if item['link'] == '#':
                        st.error(item['title']) # ì—ëŸ¬ë‚˜ë©´ ë¹¨ê°„ìƒ‰ í‘œì‹œ
                    else:
                        st.markdown(f"**[{item['title']}]({item['link']})**")
                        st.markdown("---")

        show_card(col1, "DC (48h)", dc, "ğŸ‘¿")
        show_card(col2, "MLBPARK (48h)", mlb, "ğŸŸï¸")
        show_card(col3, "FMKOREA (48h)", fmk, "âš½")

